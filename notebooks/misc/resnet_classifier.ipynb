{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-99SkxbaWtI",
        "outputId": "975f45b1-f326-4493-efb0-03557e03093e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(14, 7),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(7, 7),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(7, 7),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(7, 1),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Flatten(start_dim=0)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "M88mKVGNaWh3"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchmetrics import PrecisionRecallCurve\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "display_labels = {\"No Diabetic\", \"Yes Diabetic\"}\n",
        "\n",
        "\n",
        "\n",
        "# Seperating Features (X) AND target Variable (Y)\n",
        "data = pd.read_csv(\"cleaned_diabetes_one_hot_encoding.csv\")\n",
        "\n",
        "gk = data.groupby('diabetes')\n",
        "\n",
        "diabetes_true = gk.get_group(1)\n",
        "diabetes_false = gk.get_group(0)\n",
        "diabetes_false = diabetes_false.sample(8028)\n",
        "print(diabetes_true.shape[0])\n",
        "print(diabetes_false.shape[0])\n",
        "data = pd.concat([diabetes_false, diabetes_true])\n",
        "print(data.shape[0])\n",
        "X = data.drop(['diabetes'], axis=1)\n",
        "Y = data['diabetes']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.values).float().to(\"cpu\")\n",
        "y_train = torch.from_numpy(y_train.values).float().to(\"cpu\")\n",
        "X_val = torch.from_numpy(X_val.values).float().to(\"cpu\")\n",
        "y_val = torch.from_numpy(y_val.values).float().to(\"cpu\")\n",
        "X_test = torch.from_numpy(X_test.values).float().to(\"cpu\")\n",
        "y_test = torch.from_numpy(y_test.values).float().to(\"cpu\")\n",
        "\n",
        "batch_size=16\n",
        "epochs = round(len(X_train)/64)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# collect statistics\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "diabetes_bias=0.1\n",
        "for epoch in range(0, epochs):\n",
        "  for i in range(0, batch_size):\n",
        "        start = i * batch_size\n",
        "        # take a batch\n",
        "        Xbatch = X_train[start:start+batch_size]\n",
        "        ybatch = y_train[start:start+batch_size]\n",
        "        # forward pass\n",
        "        y_pred = model(Xbatch)\n",
        "        #print(y_pred)\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        acc = (y_pred.round() == ybatch).float().mean()\n",
        "        #metric = BinaryPrecisionRecallCurve(y_pred, ybatch)\n",
        "        y_pred_round = y_pred.round().int()\n",
        "        y_batch_int = ybatch.int()\n",
        "        true_result = torch.bitwise_and(y_pred_round, y_batch_int)\n",
        "        diff = torch.subtract(y_pred_round, y_batch_int)\n",
        "        for elm in diff:\n",
        "          if elm == -1:\n",
        "            FN += 1\n",
        "          elif elm == 1:\n",
        "            FP += 1\n",
        "          elif elm == 0:\n",
        "            TP += 1\n",
        "\n",
        "\n",
        "        train_loss.append(float(loss))\n",
        "        train_acc.append(float(acc))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Weight\n",
        "        optimizer.step()\n",
        "\n",
        "  y_pred = model(X_val)\n",
        "  acc = (y_pred.round() == y_val).float().mean()\n",
        "  val_acc.append(float(acc))\n",
        "  print(f\"End of {epoch}, accuracy {acc} Precision {TP/(TP+FP)} Recall {TP/(TP+FN)} F1 {TP/(TP+1/2*(FP+FN))}\")\n",
        "  #print(FN)\n",
        "  #print(FP)\n",
        "  #print(TP)\n",
        "  torch.save(model, \"diabettes_classifier.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze7Un6U8a_WI",
        "outputId": "da7525db-8604-4365-c0e8-20667a563818"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8028\n",
            "8028\n",
            "16056\n",
            "End of 0, accuracy 0.8767123222351074 Precision 0.9469387755102041 Recall 0.9547325102880658 F1 0.9508196721311475\n",
            "End of 1, accuracy 0.8922789692878723 Precision 0.9468302658486708 Recall 0.9526748971193416 F1 0.9497435897435897\n",
            "End of 2, accuracy 0.8829389810562134 Precision 0.9508867667121419 Recall 0.9521857923497268 F1 0.9515358361774744\n",
            "End of 3, accuracy 0.892901599407196 Precision 0.950920245398773 Recall 0.9528688524590164 F1 0.9518935516888434\n",
            "End of 4, accuracy 0.8891656398773193 Precision 0.9517184942716858 Recall 0.9524979524979525 F1 0.9521080638559148\n",
            "End of 5, accuracy 0.8891656398773193 Precision 0.9522835719154737 Recall 0.9529331514324693 F1 0.9526082509376066\n",
            "End of 6, accuracy 0.8897882699966431 Precision 0.9526869158878505 Recall 0.9532437171244886 F1 0.9529652351738241\n",
            "End of 7, accuracy 0.8885429501533508 Precision 0.9535002554931017 Recall 0.9535002554931017 F1 0.9535002554931017\n",
            "End of 8, accuracy 0.8897882699966431 Precision 0.9536784741144414 Recall 0.9536784741144414 F1 0.9536784741144414\n",
            "End of 9, accuracy 0.892901599407196 Precision 0.9529844644317252 Recall 0.9533742331288344 F1 0.9531793089347782\n",
            "End of 10, accuracy 0.8823162913322449 Precision 0.9535488665923448 Recall 0.9535488665923448 F1 0.9535488665923448\n",
            "End of 11, accuracy 0.8935242891311646 Precision 0.9529812606473594 Recall 0.9533060668029993 F1 0.9531436360538422\n",
            "End of 12, accuracy 0.8841843008995056 Precision 0.9534591194968554 Recall 0.9534591194968554 F1 0.9534591194968554\n",
            "End of 13, accuracy 0.8910336494445801 Precision 0.9529789719626168 Recall 0.95325737657026 F1 0.9531181539360304\n",
            "End of 14, accuracy 0.8897882699966431 Precision 0.9531079607415486 Recall 0.9531079607415486 F1 0.9531079607415486\n",
            "End of 15, accuracy 0.8910336494445801 Precision 0.95323281369793 Recall 0.95323281369793 F1 0.95323281369793\n",
            "End of 16, accuracy 0.8891656398773193 Precision 0.9533317296127015 Recall 0.9531024531024531 F1 0.9532170775706554\n",
            "End of 17, accuracy 0.8922789692878723 Precision 0.9531924562599409 Recall 0.9529759200363471 F1 0.9530841758491423\n",
            "End of 18, accuracy 0.8829389810562134 Precision 0.9537236332328886 Recall 0.9531081953108196 F1 0.9534158149542765\n",
            "End of 19, accuracy 0.8916562795639038 Precision 0.9533742331288344 Recall 0.9529844644317252 F1 0.9531793089347782\n",
            "End of 20, accuracy 0.8891656398773193 Precision 0.9536423841059603 Recall 0.952899961074348 F1 0.9532710280373832\n",
            "End of 21, accuracy 0.8866749405860901 Precision 0.9537088678192973 Recall 0.9530001857700168 F1 0.9533543950938487\n",
            "End of 22, accuracy 0.8922789692878723 Precision 0.9535834963542593 Recall 0.9529056335525147 F1 0.9532444444444444\n",
            "End of 23, accuracy 0.8885429501533508 Precision 0.9538094426453042 Recall 0.9528350076621829 F1 0.9533219761499149\n",
            "End of 24, accuracy 0.8866749405860901 Precision 0.9538612565445026 Recall 0.9529257927427264 F1 0.9533932951757972\n",
            "End of 25, accuracy 0.8910336494445801 Precision 0.9537444933920705 Recall 0.9528450172901604 F1 0.9532945431671647\n",
            "End of 26, accuracy 0.8835616707801819 Precision 0.9540978639600061 Recall 0.9529429565743683 F1 0.9535200605601817\n",
            "End of 27, accuracy 0.8910336494445801 Precision 0.954127100073046 Recall 0.9528742340239277 F1 0.9535002554931017\n",
            "End of 28, accuracy 0.8891656398773193 Precision 0.954295387219636 Recall 0.9528169014084507 F1 0.9535555712171401\n",
            "End of 29, accuracy 0.8872976303100586 Precision 0.9544586855740387 Recall 0.952899537163082 F1 0.9536784741144414\n",
            "End of 30, accuracy 0.8916562795639038 Precision 0.9543415149115861 Recall 0.952832674571805 F1 0.9535864978902954\n",
            "End of 31, accuracy 0.8885429501533508 Precision 0.9544873433904373 Recall 0.9527820316488004 F1 0.953633925150083\n",
            "End of 32, accuracy 0.8891656398773193 Precision 0.9546299739680179 Recall 0.9528582034149963 F1 0.953743265836894\n",
            "End of 33, accuracy 0.8904109597206116 Precision 0.9546384309950667 Recall 0.9528041311396661 F1 0.9537203990864287\n",
            "End of 34, accuracy 0.8904109597206116 Precision 0.9547632963179428 Recall 0.9527586609121661 F1 0.9537599252685661\n",
            "End of 35, accuracy 0.8848069906234741 Precision 0.9548863636363636 Recall 0.9528291189477265 F1 0.9538566320449515\n",
            "End of 36, accuracy 0.8910336494445801 Precision 0.9547816473189608 Recall 0.952885358049211 F1 0.9538325601943892\n",
            "End of 37, accuracy 0.8891656398773193 Precision 0.9548928840564108 Recall 0.9528413363411752 F1 0.9538660070975373\n",
            "End of 38, accuracy 0.8854296207427979 Precision 0.9550031466331026 Recall 0.9529042386185244 F1 0.9539525381109539\n",
            "End of 39, accuracy 0.8904109597206116 Precision 0.9549033643521833 Recall 0.9529543831003163 F1 0.953927878230667\n",
            "End of 40, accuracy 0.8904109597206116 Precision 0.9550034919684726 Recall 0.9529118964659035 F1 0.9539565477376919\n",
            "End of 41, accuracy 0.8866749405860901 Precision 0.9551032333463186 Recall 0.9529686133514722 F1 0.9540347293156282\n",
            "End of 42, accuracy 0.8904109597206116 Precision 0.9550080852278132 Recall 0.9530137636449929 F1 0.9540098821740783\n",
            "End of 43, accuracy 0.8897882699966431 Precision 0.9550990052988751 Recall 0.9529728225582043 F1 0.9540347293156282\n",
            "End of 44, accuracy 0.8866749405860901 Precision 0.9551899654608254 Recall 0.953024394667634 F1 0.9541059512460847\n",
            "End of 45, accuracy 0.8904109597206116 Precision 0.9550991375477905 Recall 0.9530653890515483 F1 0.9540811795008438\n",
            "End of 46, accuracy 0.8916562795639038 Precision 0.9551823165955966 Recall 0.9530259616219502 F1 0.9541029207232267\n",
            "End of 47, accuracy 0.8904109597206116 Precision 0.9552658486707567 Recall 0.9530731956133639 F1 0.9541682624792545\n",
            "End of 48, accuracy 0.8872976303100586 Precision 0.9553459644437026 Recall 0.9531184944624864 F1 0.9542309295539808\n",
            "End of 49, accuracy 0.8897882699966431 Precision 0.9552556237218814 Recall 0.9530727168856606 F1 0.9541629218073372\n",
            "End of 50, accuracy 0.8904109597206116 Precision 0.9553327987169206 Recall 0.953116249299944 F1 0.954223236813649\n",
            "End of 51, accuracy 0.8891656398773193 Precision 0.9554069996067637 Recall 0.9531581012161632 F1 0.9542812254516889\n",
            "End of 52, accuracy 0.8916562795639038 Precision 0.9553977930395864 Recall 0.9531177829099308 F1 0.9542564260665151\n",
            "End of 53, accuracy 0.8916562795639038 Precision 0.9554646671211089 Recall 0.9530825022665458 F1 0.9542720980369908\n",
            "End of 54, accuracy 0.8860523104667664 Precision 0.9555324211778703 Recall 0.9531226820946447 F1 0.9543260304493131\n",
            "End of 55, accuracy 0.8916562795639038 Precision 0.9554484370435291 Recall 0.9530817426781291 F1 0.9542636224378146\n",
            "End of 56, accuracy 0.8922789692878723 Precision 0.9555109070034443 Recall 0.953048955052963 F1 0.9542783431274187\n",
            "End of 57, accuracy 0.8872976303100586 Precision 0.9555743600592342 Recall 0.9530876353917569 F1 0.9543293777949928\n",
            "End of 58, accuracy 0.8916562795639038 Precision 0.9554939341421144 Recall 0.9530493707647628 F1 0.9542700868902967\n",
            "End of 59, accuracy 0.8922789692878723 Precision 0.9555525257345422 Recall 0.9530187652977972 F1 0.9542839636450284\n",
            "End of 60, accuracy 0.8872976303100586 Precision 0.9556121764784766 Recall 0.9530560385181223 F1 0.9543323958751841\n",
            "End of 61, accuracy 0.8916562795639038 Precision 0.9555350310067291 Recall 0.9530201342281879 F1 0.954275925681908\n",
            "End of 62, accuracy 0.8916562795639038 Precision 0.9555930662857884 Recall 0.9530562030562031 F1 0.9543229487470418\n",
            "End of 63, accuracy 0.8866749405860901 Precision 0.9556492842535788 Recall 0.9530911408540471 F1 0.9543684983087625\n",
            "End of 64, accuracy 0.8916562795639038 Precision 0.9555751321419582 Recall 0.9530563574745826 F1 0.9543140828253629\n",
            "End of 65, accuracy 0.8916562795639038 Precision 0.9556299188201028 Recall 0.9530902348578492 F1 0.9543583872265371\n",
            "End of 66, accuracy 0.8872976303100586 Precision 0.9556830667806129 Recall 0.9531230975283088 F1 0.9544013655206047\n",
            "End of 67, accuracy 0.8916562795639038 Precision 0.9556116925297726 Recall 0.9530893821235753 F1 0.9543488707352235\n",
            "End of 68, accuracy 0.8916562795639038 Precision 0.955663564696817 Recall 0.9531213052731142 F1 0.9543907420013614\n",
            "End of 69, accuracy 0.8879203200340271 Precision 0.9557139518579107 Recall 0.9531523132502039 F1 0.954431413734757\n",
            "End of 70, accuracy 0.8891656398773193 Precision 0.9557078677571709 Recall 0.9532372034239099 F1 0.954470936754005\n",
            "End of 71, accuracy 0.8904109597206116 Precision 0.9556969215040327 Recall 0.9532064355313845 F1 0.9544500538884793\n",
            "End of 72, accuracy 0.8879203200340271 Precision 0.9557447762030138 Recall 0.9532349983238351 F1 0.9544882374332149\n",
            "End of 73, accuracy 0.8872976303100586 Precision 0.9557360742705571 Recall 0.9532602105495233 F1 0.9544965368801568\n",
            "End of 74, accuracy 0.8910336494445801 Precision 0.95567066521265 Recall 0.9532278240060913 F1 0.9544476815421896\n",
            "End of 75, accuracy 0.8866749405860901 Precision 0.9557145931984503 Recall 0.9532013095046423 F1 0.9544562968535885\n",
            "End of 76, accuracy 0.8897882699966431 Precision 0.9557042702358189 Recall 0.9531730056150016 F1 0.9544369596350714\n",
            "End of 77, accuracy 0.8879203200340271 Precision 0.9556942114093959 Recall 0.9531454269727553 F1 0.9544181175546538\n",
            "End of 78, accuracy 0.8897882699966431 Precision 0.9556844067094636 Recall 0.9531185460553491 F1 0.9543997518353842\n",
            "End of 79, accuracy 0.8879203200340271 Precision 0.9556748466257668 Recall 0.9530923367154438 F1 0.9543818446378883\n",
            "End of 80, accuracy 0.8891656398773193 Precision 0.955665522116744 Recall 0.9530667740960822 F1 0.9543643790025718\n",
            "End of 81, accuracy 0.8885429501533508 Precision 0.9556564245810056 Recall 0.9530418345520569 F1 0.9543473387960449\n",
            "End of 82, accuracy 0.8879203200340271 Precision 0.9556475458308693 Recall 0.9530174955769609 F1 0.9543307086614173\n",
            "End of 83, accuracy 0.8910336494445801 Precision 0.9556388780677834 Recall 0.9529937357354441 F1 0.9543144739721364\n",
            "End of 84, accuracy 0.8860523104667664 Precision 0.9556785370548604 Recall 0.952972791400739 F1 0.9543237463658425\n",
            "End of 85, accuracy 0.8904109597206116 Precision 0.9556697108066972 Recall 0.9529501043445266 F1 0.9543079699819512\n",
            "End of 86, accuracy 0.8891656398773193 Precision 0.95566108707918 Recall 0.9529279384875052 F1 0.9542925558137897\n",
            "End of 87, accuracy 0.8885429501533508 Precision 0.9556526589810338 Recall 0.9529062760730509 F1 0.9542774915285708\n",
            "End of 88, accuracy 0.8897882699966431 Precision 0.9556444199301343 Recall 0.9528851001420781 F1 0.9542627653471027\n",
            "End of 89, accuracy 0.8904109597206116 Precision 0.9556363636363636 Recall 0.9528643944887599 F1 0.954248366013072\n",
            "End of 90, accuracy 0.8885429501533508 Precision 0.9556284840855961 Recall 0.9528441436191671 F1 0.9542342827643481\n",
            "End of 91, accuracy 0.8891656398773193 Precision 0.9556207755247242 Recall 0.9528243327126009 F1 0.9542205053061587\n",
            "End of 92, accuracy 0.8872976303100586 Precision 0.9556132324476508 Recall 0.9528049475854204 F1 0.954207023785992\n",
            "End of 93, accuracy 0.8904109597206116 Precision 0.9556058495821727 Recall 0.9527859746571776 F1 0.9541938287701\n",
            "End of 94, accuracy 0.8854296207427979 Precision 0.9555986218776916 Recall 0.9527674009188888 F1 0.954180911221484\n",
            "End of 95, accuracy 0.8897882699966431 Precision 0.9555915444936924 Recall 0.9527492139032888 F1 0.9541682624792545\n",
            "End of 96, accuracy 0.8904109597206116 Precision 0.955584612788932 Recall 0.9527314016569242 F1 0.9541558742392655\n",
            "End of 97, accuracy 0.8872976303100586 Precision 0.9555778223112893 Recall 0.9527139527139528 F1 0.9541437385359346\n",
            "End of 98, accuracy 0.8904109597206116 Precision 0.9555711687882295 Recall 0.9526968560715315 F1 0.9541318477251625\n",
            "End of 99, accuracy 0.8848069906234741 Precision 0.9555646481178396 Recall 0.9526801011666802 F1 0.9541201944682763\n",
            "End of 100, accuracy 0.8891656398773193 Precision 0.9555582563603954 Recall 0.9526636778545176 F1 0.9541087717169265\n",
            "End of 101, accuracy 0.8891656398773193 Precision 0.9555519897304237 Recall 0.952647576387778 F1 0.9540975726988704\n",
            "End of 102, accuracy 0.8891656398773193 Precision 0.9555476105350972 Recall 0.9526713929264525 F1 0.9541073341001943\n",
            "End of 103, accuracy 0.8891656398773193 Precision 0.9555415666679781 Recall 0.9526555267906174 F1 0.9540963642434838\n",
            "End of 104, accuracy 0.8872976303100586 Precision 0.9555356377382019 Recall 0.9526399627025137 F1 0.9540856031128405\n",
            "End of 105, accuracy 0.8897882699966431 Precision 0.9555298204979734 Recall 0.9526246921182266 F1 0.9540750448071845\n",
            "End of 106, accuracy 0.8891656398773193 Precision 0.9555241118207197 Recall 0.9526097068130696 F1 0.9540646836458055\n",
            "End of 107, accuracy 0.8885429501533508 Precision 0.9555185086954875 Recall 0.9525949988668128 F1 0.9540545141581706\n",
            "End of 108, accuracy 0.8885429501533508 Precision 0.9555130082216465 Recall 0.952580560649725 F1 0.9540445310742934\n",
            "End of 109, accuracy 0.8891656398773193 Precision 0.9555076076038838 Recall 0.9525663848093755 F1 0.9540347293156282\n",
            "End of 110, accuracy 0.8885429501533508 Precision 0.9555023041474654 Recall 0.9525524642581499 F1 0.9540251039864541\n",
            "End of 111, accuracy 0.8891656398773193 Precision 0.9554970952537543 Recall 0.9525387921614337 F1 0.9540156503657224\n",
            "End of 112, accuracy 0.8891656398773193 Precision 0.9554919784159634 Recall 0.9525253619264233 F1 0.9540063638993347\n",
            "End of 113, accuracy 0.8885429501533508 Precision 0.9554869512151344 Recall 0.9525121671915259 F1 0.953997240192828\n",
            "End of 114, accuracy 0.8916562795639038 Precision 0.9554820113163233 Recall 0.9524992018163113 F1 0.9539882750044413\n",
            "End of 115, accuracy 0.8860523104667664 Precision 0.955477156464985 Recall 0.9524864598719842 F1 0.9539794642385389\n",
            "End of 116, accuracy 0.8885429501533508 Precision 0.9554723844835427 Recall 0.9524739356323442 F1 0.9539708039393727\n",
            "End of 117, accuracy 0.8885429501533508 Precision 0.9554676932681303 Recall 0.9524616235652054 F1 0.9539622902851602\n",
            "End of 118, accuracy 0.8860523104667664 Precision 0.9554646124217622 Recall 0.9524838012958964 F1 0.9539718783800024\n",
            "End of 119, accuracy 0.8885429501533508 Precision 0.9554585450700863 Recall 0.9524376147412796 F1 0.9539456882608326\n",
            "End of 120, accuracy 0.8922789692878723 Precision 0.9554540842212075 Recall 0.9524259078188746 F1 0.9539375928677564\n",
            "End of 121, accuracy 0.8841843008995056 Precision 0.955449696400416 Recall 0.9524143927233815 F1 0.9539296300638052\n",
            "End of 122, accuracy 0.8885429501533508 Precision 0.9554453798289688 Recall 0.952403064778268 F1 0.9539217966180525\n",
            "End of 123, accuracy 0.8885429501533508 Precision 0.9554411327854243 Recall 0.9523919194577877 F1 0.9539140894037007\n",
            "End of 124, accuracy 0.8885429501533508 Precision 0.9554384126776243 Recall 0.9524135905218839 F1 0.9539236037331852\n",
            "End of 125, accuracy 0.8885429501533508 Precision 0.9554342883128695 Recall 0.9524025385312783 F1 0.9539160045402951\n",
            "End of 126, accuracy 0.8885429501533508 Precision 0.9554302288108283 Recall 0.9523916605094928 F1 0.9539085249119195\n",
            "End of 127, accuracy 0.8885429501533508 Precision 0.9554262326533223 Recall 0.9523809523809523 F1 0.9539011620482697\n",
            "End of 128, accuracy 0.8879203200340271 Precision 0.9554222983691859 Recall 0.9523704101964009 F1 0.9538939132362957\n",
            "End of 129, accuracy 0.8860523104667664 Precision 0.9554198281018795 Recall 0.9523914135074065 F1 0.9539032171876719\n",
            "End of 130, accuracy 0.8897882699966431 Precision 0.9554146097606698 Recall 0.9523498084649163 F1 0.9538797473290181\n",
            "End of 131, accuracy 0.8879203200340271 Precision 0.9554108527131783 Recall 0.9523397416084565 F1 0.9538728252120612\n",
            "End of 132, accuracy 0.8879203200340271 Precision 0.9554085243883674 Recall 0.9523605018558852 F1 0.9538820782253357\n",
            "End of 133, accuracy 0.8879203200340271 Precision 0.9554048688108984 Recall 0.9523505054195591 F1 0.9538752420596801\n",
            "End of 134, accuracy 0.8879203200340271 Precision 0.9554012673195282 Recall 0.9523406570159267 F1 0.9538685070831819\n",
            "End of 135, accuracy 0.8885429501533508 Precision 0.9553977187227255 Recall 0.9523309533809324 F1 0.953861871065639\n",
            "End of 136, accuracy 0.8879203200340271 Precision 0.9553942218637029 Recall 0.9523213913457816 F1 0.9538553318419091\n",
            "End of 137, accuracy 0.8854296207427979 Precision 0.9553920987068454 Recall 0.9523415326395459 F1 0.9538643766656796\n",
            "End of 138, accuracy 0.8897882699966431 Precision 0.955387378898083 Recall 0.9523026798555871 F1 0.953842535426589\n",
            "End of 139, accuracy 0.8885429501533508 Precision 0.9553840306405871 Recall 0.9522935245089468 F1 0.9538362742093203\n",
            "End of 140, accuracy 0.8885429501533508 Precision 0.9553820250812819 Recall 0.9523134349952256 F1 0.953845262074862\n",
            "End of 141, accuracy 0.8879203200340271 Precision 0.9553787616741611 Recall 0.9523043328353062 F1 0.9538390698745252\n",
            "End of 142, accuracy 0.8879203200340271 Precision 0.9553755438516144 Recall 0.952295357927473 F1 0.9538329642066157\n",
            "End of 143, accuracy 0.8879203200340271 Precision 0.9553723706651507 Recall 0.9522865076216921 F1 0.9538269432698584\n",
            "End of 144, accuracy 0.8879203200340271 Precision 0.9553705010585745 Recall 0.9523059174428093 F1 0.9538357477030607\n",
            "End of 145, accuracy 0.8879203200340271 Precision 0.9553686571348472 Recall 0.9523250614799911 F1 0.9538444313824278\n",
            "End of 146, accuracy 0.8879203200340271 Precision 0.9553655955894638 Recall 0.9523161897360459 F1 0.9538384554438931\n",
            "End of 147, accuracy 0.8872976303100586 Precision 0.9553625753636816 Recall 0.9523074378342614 F1 0.9538325601943892\n",
            "End of 148, accuracy 0.8872976303100586 Precision 0.9553608219102822 Recall 0.9523261863687396 F1 0.9538410904802391\n",
            "End of 149, accuracy 0.8879203200340271 Precision 0.9553590919013316 Recall 0.9523446850179523 F1 0.9538495068926061\n",
            "End of 150, accuracy 0.8904109597206116 Precision 0.9553561747804402 Recall 0.9523359182901456 F1 0.9538436557015466\n",
            "End of 151, accuracy 0.8829389810562134 Precision 0.955354498209333 Recall 0.952354109625812 F1 0.9538519444556465\n",
            "End of 152, accuracy 0.8910336494445801 Precision 0.955350454788657 Recall 0.9523187284994267 F1 0.9538321825879085\n",
            "End of 153, accuracy 0.8835616707801819 Precision 0.9553488372093023 Recall 0.9523367952522255 F1 0.9538404383765209\n",
            "End of 154, accuracy 0.8897882699966431 Precision 0.955344882222457 Recall 0.9523019821527284 F1 0.9538210053126277\n",
            "End of 155, accuracy 0.8879203200340271 Precision 0.9553421494542401 Recall 0.9522937699429826 F1 0.9538155240615095\n",
            "End of 156, accuracy 0.8848069906234741 Precision 0.9553406157937273 Recall 0.9523116505106681 F1 0.9538237284606175\n",
            "End of 157, accuracy 0.8891656398773193 Precision 0.9553367875647668 Recall 0.9522776572668112 F1 0.9538047695411516\n",
            "End of 158, accuracy 0.8866749405860901 Precision 0.955334157141386 Recall 0.9522697528804948 F1 0.9537994936578721\n",
            "End of 159, accuracy 0.8848069906234741 Precision 0.9553327022947632 Recall 0.9522874483602795 F1 0.9538076446624012\n",
            "End of 160, accuracy 0.8885429501533508 Precision 0.9553289942031933 Recall 0.9522542385767505 F1 0.9537891383533652\n",
            "End of 161, accuracy 0.8866749405860901 Precision 0.9553264604810997 Recall 0.9522466250251864 F1 0.9537840565085772\n",
            "End of 162, accuracy 0.8841843008995056 Precision 0.9553250797317998 Recall 0.9522641367743874 F1 0.95379215243826\n",
            "End of 163, accuracy 0.8916562795639038 Precision 0.9553214856230032 Recall 0.9522316763696074 F1 0.9537740785965262\n",
            "End of 164, accuracy 0.8823162913322449 Precision 0.9553449601825895 Recall 0.9522502472799209 F1 0.9537950934376896\n",
            "End of 165, accuracy 0.8872976303100586 Precision 0.9553423914383646 Recall 0.952242841342018 F1 0.9537900982298925\n",
            "End of 166, accuracy 0.8904109597206116 Precision 0.9553387587018335 Recall 0.9522110921084779 F1 0.9537723613048479\n",
            "End of 167, accuracy 0.8835616707801819 Precision 0.9553617114592724 Recall 0.9522294540509034 F1 0.953793011177737\n",
            "End of 168, accuracy 0.8910336494445801 Precision 0.9553590892575996 Recall 0.9522223026966997 F1 0.9537881169443572\n",
            "End of 169, accuracy 0.8854296207427979 Precision 0.9553575728389117 Recall 0.9522392358277733 F1 0.9537958555699794\n",
            "End of 170, accuracy 0.8891656398773193 Precision 0.955355005505817 Recall 0.9522321109016726 F1 0.9537910019716795\n",
            "End of 171, accuracy 0.8860523104667664 Precision 0.9553762673140083 Recall 0.9522262020542259 F1 0.9537986337986338\n",
            "End of 172, accuracy 0.8848069906234741 Precision 0.9553746776139886 Recall 0.9522428187349653 F1 0.9538061772869545\n",
            "End of 173, accuracy 0.8891656398773193 Precision 0.9553710064461488 Recall 0.9522123478791005 F1 0.9537890620412669\n",
            "End of 174, accuracy 0.8860523104667664 Precision 0.9553918128654971 Recall 0.9522066537663488 F1 0.9537965741508354\n",
            "End of 175, accuracy 0.8885429501533508 Precision 0.9553901616467031 Recall 0.9522230979646715 F1 0.9538040007894767\n",
            "End of 176, accuracy 0.8854296207427979 Precision 0.9554116558741905 Recall 0.9522404573114512 F1 0.95382342076099\n",
            "End of 177, accuracy 0.8885429501533508 Precision 0.9554088860270444 Recall 0.9522336060876939 F1 0.9538186034231401\n",
            "End of 178, accuracy 0.8872976303100586 Precision 0.955429015733626 Recall 0.9522279202279202 F1 0.95382578222207\n",
            "End of 179, accuracy 0.8848069906234741 Precision 0.9554271939599299 Recall 0.9522438803263826 F1 0.9538328811596836\n",
            "End of 180, accuracy 0.8891656398773193 Precision 0.9554233761534286 Recall 0.952214583568128 F1 0.9538162811438119\n",
            "End of 181, accuracy 0.8860523104667664 Precision 0.95544409707384 Recall 0.9522315124072538 F1 0.9538350996946291\n",
            "End of 182, accuracy 0.8879203200340271 Precision 0.9554422225204671 Recall 0.952247191011236 F1 0.9538420312185971\n",
            "End of 183, accuracy 0.8854296207427979 Precision 0.9554626148472781 Recall 0.9522637577047581 F1 0.953860504369649\n",
            "End of 184, accuracy 0.8885429501533508 Precision 0.9554596747427813 Recall 0.9522570401570115 F1 0.9538556691921981\n",
            "End of 185, accuracy 0.8879203200340271 Precision 0.9554797535211268 Recall 0.9522733752988397 F1 0.9538738699153054\n",
            "End of 186, accuracy 0.8848069906234741 Precision 0.9554777279194484 Recall 0.9522884942624024 F1 0.9538804453525344\n",
            "End of 187, accuracy 0.8872976303100586 Precision 0.9554747545123996 Recall 0.9522817524900723 F1 0.9538755814458983\n",
            "End of 188, accuracy 0.8879203200340271 Precision 0.9554944340971109 Recall 0.9522976969069049 F1 0.9538933872414949\n",
            "End of 189, accuracy 0.8860523104667664 Precision 0.9554923630409962 Recall 0.9523124490058831 F1 0.953899755895605\n",
            "End of 190, accuracy 0.8860523104667664 Precision 0.9554893598782761 Recall 0.9523056878617655 F1 0.9538948674611155\n",
            "End of 191, accuracy 0.8872976303100586 Precision 0.9554854389630324 Recall 0.9522777494475607 F1 0.9538788975204853\n",
            "End of 192, accuracy 0.8866749405860901 Precision 0.9555046552565163 Recall 0.952293384062566 F1 0.9538963169985497\n",
            "End of 193, accuracy 0.8854296207427979 Precision 0.9555025740568824 Recall 0.9523078540637157 F1 0.9539025392044318\n",
            "End of 194, accuracy 0.8885429501533508 Precision 0.9554986460672978 Recall 0.9522803347280334 F1 0.9538867758463522\n",
            "End of 195, accuracy 0.8866749405860901 Precision 0.9555175006265141 Recall 0.9522957166049203 F1 0.9539038882518504\n",
            "End of 196, accuracy 0.8879203200340271 Precision 0.9555153857341727 Recall 0.9523099542357789 F1 0.9539099771831571\n",
            "End of 197, accuracy 0.8860523104667664 Precision 0.9555339645263985 Recall 0.9523250303891876 F1 0.953926798815408\n",
            "End of 198, accuracy 0.8860523104667664 Precision 0.9555308733390925 Recall 0.9523184781271781 F1 0.9539219712525667\n",
            "End of 199, accuracy 0.8872976303100586 Precision 0.9555269028468513 Recall 0.9522915944275603 F1 0.9539065053939196\n",
            "End of 200, accuracy 0.8860523104667664 Precision 0.9555451472325174 Recall 0.952306537048688 F1 0.9539230933430916\n"
          ]
        }
      ]
    }
  ]
}